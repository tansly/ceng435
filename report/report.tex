\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}


\title{CENG435 Term Project, Part 1\\
}

\author{\IEEEauthorblockN{Doruk Coskun}
\IEEEauthorblockA{
}
\and
\IEEEauthorblockN{Yagmur Oymak}
\IEEEauthorblockA{
}
}

\maketitle

\begin{abstract}
\end{abstract}

\begin{IEEEkeywords}
ceng, networking, delay
\end{IEEEkeywords}

\section{Introduction}

In our CENG435 Term Project Part 1 we have designed a network topology according to the information provided to us in the homework description. We have formed a TCP connection between Source and Broker. We implemented UDP connection between Broker, Router 1, Router 2 and Destination. We tested packet loss and packet delays in our network.

\section{Setting Up the Environment}
To set network emulation delays, \textbf{tc} command is used. \textbf{tc} manipulates
traffic control parameters in a Linux system. Running verbose \textbf{tc} commands
in multiple machines becomes tedious very quickly.
In order to quickly and conveniently set delays in multiple nodes and multiple links,
we've used the shell scripts mentioned in the README file.

Likewise, short shell scripts were used for placing the code files in relevant
nodes.

\section{Network Topology Design Decisions}

In our design, packet sent from Source passes through Broker, Router 1 and reaches Destination. All scripts except Broker is written in Python3. Before the packet is sent a TCP connection formed between Source and Broker. Our sensor readings are 8 bytes long and consists of the index of the packet. We have chosen this packet size so that we can can minimize the transmission delay and observe the netem/tc delays better. When the byte stream reaches Broker, they are sent to Router 1 through UDP connection. Router 1 listens to a predefined port number and directs the packet to Destination. Destination prints out the content of the received packet and sends feedback back to the source through Router 2. Router 2 directs the packet received from Destination to Broker. Finally broker uses the already open TCP connection to send the feedback message to Source. Source calculates the time difference between the sent packet and the received feedback message. By this method we have calculated the round-trip time. We have also implemented a method where we have synchronized the nodes and calculated the one way delay using time stamps that are sent with the packets. But we believe we have gathered more accurate results using round-trip time. You can find detailed synchronization implementation in the following parts of this documentation.

\subsection{Test 1: Packet Loss Test}\label{AA}

Apart from delay tests we have also implemented packet loss test. In this test we have sent 1000 consecutive packets from Source and observed how many of them reached to Broker and then to Destination. Its important to keep in mind that our packet size is 8 bytes. 
\begin{itemize}
\item 1000/1000: All of the packets sent by Source were received by Broker. Since the connection between Source and Destination is TCP, packet loss is not expected.
\item 67/1000: Only 67 of the packets were recieved by the destination. There is a significant packet loss. We exceed the capacity of the links between the other nodes. That's why while transfering the packets through UDP nodes, most of them were lost. While we were sending packets of size 128 bytes, the packet loss rate was even higher, with only 17 packets reaching Destination. We also observed that when we increased the size of the packets to 4 times, the amount of the packets delivered to Destination reduced to 1 over 4.
\end{itemize}

\subsection{Detailed Explanation of Synchronized Nodes}
As previously mentioned, we approximate the end-to-end delay using the round trip time, i.e.\
end-to-end delay is round trip time divided by two. Since our network delays will
most likely be symmetric, this approach deemed to be accurate enough. Still,
we implmented an alternative method of calculating the end-to-end delay using
timestamps in packets. For this approach to produce reliable results, the clocks
of the source and the destination must be synchronized.

In order to synchronize the nodes, \textbf{NTP} is used. NTP sets the clock of the
node using information from a time server. The following command is used to set the
clock of the node:
\begin{lstlisting}
sudo ntpdate -s time.nist.gov
\end{lstlisting}
Then, the \textbf{clockdiff} program can be used to calculate the clock difference
of two nodes. \textbf{clockdiff} uses ICMP TIMESTAMP packets (RFC0792, page 16)
to measure the clock difference of two hosts with a 1ms resolution. An example run:
\begin{lstlisting}
yagmuroy@d:~\$ clockdiff 172.17.1.7.
host=172.17.1.7 rtt=750(187)ms/0ms
delta=-277ms/-277ms Wed Nov 28 11:55:06 2018
\end{lstlisting}
When the clock difference (delta) became small (around 5ms), we ran our experiments.
Not to our surprise, the measured delay value was very close to our other approximation
(round trip time divided by two). For practical purposes\footnote{Even though the nodes had been
synchronised, their clocks started to drift and introduced noise in our measurements
before any meaningful data was collected.}, we deemed it appropriate
to use the round trip time approximation instead of the timestamp method.

\subsection{Detailed Explanation of Broker Implementation}
The broker is implemented as a multi-process server in C. This section describes
the implementation of the broker from a high level viewpoint.

The broker, when started, creates a TCP socket, binds it to a local port, and listens
it. It also creates two UDP sockets, one to send datagrams to Router 1, and one to receive
datagrams from Router 2. Since the first UDP socket will always send datagrams to a
fixed address and port (identifying the receiving socket at Router 1), we also
call connect on the socket to save us from specifying the address and port at every send call.
The second will receive datagrams from Router 2, therefore it must wait for packets
on a fixed port (the port that Router 2 will send to). So it also binds the socket
to a local address and port.

After the sockets are created, connected, bound and listened, the broker starts
its main loop in which it accepts TCP connections with the accept system call.
When the Source connects to the broker, a new worker process is spawned. This worker
process will handle the TCP connection socket. It receives byte streams from the Source,
packetizes the bytes and sends them to Router 1 using the previously
created UDP sockets. It also spawns another worker process (which shares the connection
socket) that receives datagrams (containing feedback messages) from Router 2 and
sends them over the TCP connection socket to the Source.

When the TCP connection from the source is terminated, the recv call will return 0
to the broker indicating connection termination. Knowing the connection is closed,
the worker process kills its child (the other worker process that it spawned),
checks if there is any leftover data coming from Router 2, clears it if there is
and exits. The main process still waits to accept new TCP connections, so the brokes
is a server that always keeps running.

\section{Experiments}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{graphics/plt}
    \caption{killa}
\end{figure}

\end{document}
